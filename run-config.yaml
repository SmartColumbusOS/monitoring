externalDns:
  args:
    source: ingress
    provider: aws
    policy: upsert-only
    aws-zone-type: public
    registry: txt
    txt-owner-id: scos
    # domain-filter: ${DNS_ZONE}

grafana:
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations:
      alb.ingress.kubernetes.io/healthcheck-path: /api/health
  persistentVolume:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 5Gi
  datasources: 
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus-server.prometheus.svc.cluster.local
        access: proxy
        isDefault: true
  dashboards:
    default:
      kube-official-dash:
        gnetId: 2
        revision: 2
        datasource: Prometheus
      kube-nodes:
        gnetId: 7692
        revision: 1
        datasource: Prometheus
      kube-namespace:
        gnetId: 6876
        revision: 2
        datasource: Prometheus
      kube-cluster:
        gnetId: 6873
        revision: 2
        datasource: Prometheus
      kube-pods:
        gnetId: 6879
        revision: 1
        datasource: Prometheus
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default

alertmanager:
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations:
      alb.ingress.kubernetes.io/healthcheck-path: /-/healthy

server:
  service:
    type: NodePort
  ingress:
    enabled: true
    annotations:
      alb.ingress.kubernetes.io/healthcheck-path: /-/healthy

serverFiles:
  alerts:
    groups:
      - name: Instances
        rules:
          - alert: InstanceDown
            expr: up == 0
            for: 2m
            labels:
              severity: page
            annotations:
              description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
              summary: 'Instance {{ $labels.instance }} down'
      # Blackbox probe
      - name: Sites
        rules:
          - alert: SiteDown
            expr: probe_success == 0
            for: 2m
            annotations:
              description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
              summary: 'Instance {{ $labels.instance }} down'

# I would like the below config to look like this!
# blackbox:
#   targets: #Add sites to monitor here
#     - https://jupyter.dev.internal.smartcolumbusos.com/hub/login
#     - https://cota.dev.internal.smartcolumbusos.com
#     - https://streaming.dev.internal.smartcolumbusos.com/socket/healthcheck
#   hostname: prometheus-prometheus-blackbox-exporter:9115

extraScrapeConfigs: |
  - job_name: 'blackbox'
    metrics_path: /probe
    params:
      module: [http_2xx] # Look for a HTTP 200 response.
    static_configs:
      - targets: # Add sites to monitor here
        - https://jupyter.dev.internal.smartcolumbusos.com/hub/login
        - https://cota.dev.internal.smartcolumbusos.com
        - https://streaming.dev.internal.smartcolumbusos.com/socket/healthcheck
        - https://ckan.dev.internal.smartcolumbusos.com/
        - https://www.dev.internal.smartcolumbusos.com/
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        # ${namespace}-prometheus-blackbox-exporter:9115
        replacement: prometheus-prometheus-blackbox-exporter:9115

global:
  ingress:
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/tags: scos.delete.on.teardown=true
      alb.ingress.kubernetes.io/scheme: internal

# helm install --name=prometheus --namespace=prometheus \
#   --set global.ingress.annotations."alb\.ingress\.kubernetes\.io\/subnets"="${SUBNETS//,/\\,}" \
#   --set global.ingress.annotations."alb\.ingress\.kubernetes\.io\/security\-groups"="${SECURITY_GROUPS}" \
#   --set grafana.ingress.hosts[0]="grafana\.${DNS_ZONE}" \
#   --set alertmanager.ingress.hosts[0]="alertmanager\.${DNS_ZONE}" \
#   --set server.ingress.hosts[0]="prometheus\.${DNS_ZONE}" \
#   --values prometheus/run-config.yaml prometheus